{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-WzjZiO3t_A"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Define the path to the directory containing the JPG and XML files\n",
    "train_directory_path = r'/content/drive/MyDrive/thesis/dataset/roboflow-cnn/train'\n",
    "valid_directory_path = r'/content/drive/MyDrive/thesis/dataset/roboflow-cnn/valid'\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Define the input shape of the model\n",
    "input_shape = (224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6810,
     "status": "ok",
     "timestamp": 1690615688754,
     "user": {
      "displayName": "TT CR",
      "userId": "00594919096815658785"
     },
     "user_tz": -360
    },
    "id": "8Sh_58Wlim0-",
    "outputId": "cf599016-ddeb-4845-f53b-9eaadd8e1e17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load the VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "# Add a custom output layer for bounding box regression (4 values)\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(4, activation='linear')(x)  # 4 values for xmin, ymin, xmax, ymax\n",
    "model = Model(inputs=base_model.input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_NZ6wpciot2"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
    "save_best = tf.keras.callbacks.ModelCheckpoint(\"/content/drive/MyDrive/thesis/cnn_models/VGG16Modelv1.h5\", monitor='accuracy', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z6GTI6YEi2O8"
   },
   "outputs": [],
   "source": [
    "# Define a data generator that yields batches of images and their corresponding annotations\n",
    "def data_generator(directory_path, batch_size, input_shape):\n",
    "    while True:\n",
    "        data = []\n",
    "        train_annotation = []\n",
    "        for filename in os.listdir(directory_path):\n",
    "            if filename.endswith('.jpg'):\n",
    "                # Extract the file path of the JPG image\n",
    "                image_path = os.path.join(directory_path, filename)\n",
    "                # Construct the file path of the corresponding XML annotation file\n",
    "                xml_path = os.path.join(directory_path, filename[:-4] + '.xml')\n",
    "\n",
    "                # Parse the XML annotation file\n",
    "                tree = ET.parse(xml_path)\n",
    "                root = tree.getroot()\n",
    "                # Check if the bndbox element is present\n",
    "                bndbox = root.find('object/bndbox')\n",
    "                if bndbox is not None:\n",
    "                    # Extract the bounding box coordinates\n",
    "                    xmin = int(bndbox.find('xmin').text)\n",
    "                    xmax = int(bndbox.find('xmax').text)\n",
    "                    ymin = int(bndbox.find('ymin').text)\n",
    "                    ymax = int(bndbox.find('ymax').text)\n",
    "                    # Normalize the bounding box coordinates to the range [0, 1]\n",
    "                    train_annotation.append([xmin / input_shape[0], ymin / input_shape[1],\n",
    "                                             xmax / input_shape[0], ymax / input_shape[1]])\n",
    "\n",
    "                    # Load the image\n",
    "                    image = cv2.resize(cv2.imread(image_path), input_shape[:2])\n",
    "                    data.append(image)\n",
    "\n",
    "                    # Yield the batch when the data and train_annotation lists are of length batch_size\n",
    "                    if len(data) == batch_size:\n",
    "                        yield np.array(data), np.array(train_annotation)\n",
    "                        data = []\n",
    "                        train_annotation = []\n",
    "\n",
    "        # Yield the last batch if it is smaller than batch_size\n",
    "        if len(data) > 0:\n",
    "            yield np.array(data), np.array(train_annotation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1642809,
     "status": "ok",
     "timestamp": 1690618040778,
     "user": {
      "displayName": "TT CR",
      "userId": "00594919096815658785"
     },
     "user_tz": -360
    },
    "id": "k81OB_6Mi3f8",
    "outputId": "94009414-593e-49bb-ee2d-e4f4bda1754f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.4589 - accuracy: 0.5759\n",
      "Epoch 1: accuracy improved from -inf to 0.57587, saving model to /content/drive/MyDrive/thesis/cnn_models/VGG16Modelv1.h5\n",
      "123/123 [==============================] - 87s 706ms/step - loss: 0.4589 - accuracy: 0.5759 - val_loss: 0.1466 - val_accuracy: 0.5666\n",
      "Epoch 2/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.1167 - accuracy: 0.6559\n",
      "Epoch 2: accuracy improved from 0.57587 to 0.65591, saving model to /content/drive/MyDrive/thesis/cnn_models/VGG16Modelv1.h5\n",
      "123/123 [==============================] - 65s 531ms/step - loss: 0.1167 - accuracy: 0.6559 - val_loss: 0.1226 - val_accuracy: 0.6113\n",
      "Epoch 3/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0995 - accuracy: 0.6959\n",
      "Epoch 3: accuracy improved from 0.65591 to 0.69585, saving model to /content/drive/MyDrive/thesis/cnn_models/VGG16Modelv1.h5\n",
      "123/123 [==============================] - 64s 519ms/step - loss: 0.0995 - accuracy: 0.6959 - val_loss: 0.1162 - val_accuracy: 0.5970\n",
      "Epoch 4/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0887 - accuracy: 0.7273\n",
      "Epoch 4: accuracy improved from 0.69585 to 0.72734, saving model to /content/drive/MyDrive/thesis/cnn_models/VGG16Modelv1.h5\n",
      "123/123 [==============================] - 64s 520ms/step - loss: 0.0887 - accuracy: 0.7273 - val_loss: 0.1144 - val_accuracy: 0.5853\n",
      "Epoch 5/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0780 - accuracy: 0.7440\n",
      "Epoch 5: accuracy improved from 0.72734 to 0.74398, saving model to /content/drive/MyDrive/thesis/cnn_models/VGG16Modelv1.h5\n",
      "123/123 [==============================] - 64s 521ms/step - loss: 0.0780 - accuracy: 0.7440 - val_loss: 0.1058 - val_accuracy: 0.6810\n",
      "Epoch 6/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.7640\n",
      "Epoch 6: accuracy improved from 0.74398 to 0.76395, saving model to /content/drive/MyDrive/thesis/cnn_models/VGG16Modelv1.h5\n",
      "123/123 [==============================] - 66s 540ms/step - loss: 0.0683 - accuracy: 0.7640 - val_loss: 0.0995 - val_accuracy: 0.7167\n",
      "Epoch 7/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0610 - accuracy: 0.7762\n",
      "Epoch 7: accuracy improved from 0.76395 to 0.77624, saving model to /content/drive/MyDrive/thesis/cnn_models/VGG16Modelv1.h5\n",
      "123/123 [==============================] - 64s 523ms/step - loss: 0.0610 - accuracy: 0.7762 - val_loss: 0.0868 - val_accuracy: 0.7337\n",
      "Epoch 8/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0536 - accuracy: 0.8008\n",
      "Epoch 8: accuracy improved from 0.77624 to 0.80082, saving model to /content/drive/MyDrive/thesis/cnn_models/VGG16Modelv1.h5\n",
      "123/123 [==============================] - 65s 525ms/step - loss: 0.0536 - accuracy: 0.8008 - val_loss: 0.0917 - val_accuracy: 0.7256\n",
      "Epoch 9/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0503 - accuracy: 0.8159\n",
      "Epoch 9: accuracy improved from 0.80082 to 0.81592, saving model to /content/drive/MyDrive/thesis/cnn_models/VGG16Modelv1.h5\n",
      "123/123 [==============================] - 64s 523ms/step - loss: 0.0503 - accuracy: 0.8159 - val_loss: 0.1003 - val_accuracy: 0.7105\n",
      "Epoch 10/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0503 - accuracy: 0.8152\n",
      "Epoch 10: accuracy did not improve from 0.81592\n",
      "123/123 [==============================] - 64s 520ms/step - loss: 0.0503 - accuracy: 0.8152 - val_loss: 0.0944 - val_accuracy: 0.7203\n",
      "Epoch 11/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.8175\n",
      "Epoch 11: accuracy improved from 0.81592 to 0.81746, saving model to /content/drive/MyDrive/thesis/cnn_models/VGG16Modelv1.h5\n",
      "123/123 [==============================] - 61s 497ms/step - loss: 0.0484 - accuracy: 0.8175 - val_loss: 0.1017 - val_accuracy: 0.7060\n",
      "Epoch 12/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0427 - accuracy: 0.8251\n",
      "Epoch 12: accuracy improved from 0.81746 to 0.82514, saving model to /content/drive/MyDrive/thesis/cnn_models/VGG16Modelv1.h5\n",
      "123/123 [==============================] - 63s 514ms/step - loss: 0.0427 - accuracy: 0.8251 - val_loss: 0.0934 - val_accuracy: 0.6819\n",
      "Epoch 13/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.8377\n",
      "Epoch 13: accuracy improved from 0.82514 to 0.83769, saving model to /content/drive/MyDrive/thesis/cnn_models/VGG16Modelv1.h5\n",
      "123/123 [==============================] - 61s 500ms/step - loss: 0.0392 - accuracy: 0.8377 - val_loss: 0.0984 - val_accuracy: 0.6890\n",
      "Epoch 14/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0369 - accuracy: 0.8374\n",
      "Epoch 14: accuracy did not improve from 0.83769\n",
      "123/123 [==============================] - 61s 500ms/step - loss: 0.0369 - accuracy: 0.8374 - val_loss: 0.0944 - val_accuracy: 0.7033\n",
      "Epoch 15/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0352 - accuracy: 0.8354\n",
      "Epoch 15: accuracy did not improve from 0.83769\n",
      "123/123 [==============================] - 62s 507ms/step - loss: 0.0352 - accuracy: 0.8354 - val_loss: 0.0866 - val_accuracy: 0.7185\n",
      "Epoch 16/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.8602\n",
      "Epoch 16: accuracy improved from 0.83769 to 0.86022, saving model to /content/drive/MyDrive/thesis/cnn_models/VGG16Modelv1.h5\n",
      "123/123 [==============================] - 66s 537ms/step - loss: 0.0325 - accuracy: 0.8602 - val_loss: 0.0933 - val_accuracy: 0.7605\n",
      "Epoch 17/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 0.8513\n",
      "Epoch 17: accuracy did not improve from 0.86022\n",
      "123/123 [==============================] - 59s 481ms/step - loss: 0.0322 - accuracy: 0.8513 - val_loss: 0.0799 - val_accuracy: 0.7426\n",
      "Epoch 18/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.8620\n",
      "Epoch 18: accuracy improved from 0.86022 to 0.86201, saving model to /content/drive/MyDrive/thesis/cnn_models/VGG16Modelv1.h5\n",
      "123/123 [==============================] - 66s 537ms/step - loss: 0.0262 - accuracy: 0.8620 - val_loss: 0.0827 - val_accuracy: 0.7507\n",
      "Epoch 19/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.8705\n",
      "Epoch 19: accuracy improved from 0.86201 to 0.87046, saving model to /content/drive/MyDrive/thesis/cnn_models/VGG16Modelv1.h5\n",
      "123/123 [==============================] - 65s 529ms/step - loss: 0.0227 - accuracy: 0.8705 - val_loss: 0.0744 - val_accuracy: 0.7694\n",
      "Epoch 20/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.8676\n",
      "Epoch 20: accuracy did not improve from 0.87046\n",
      "123/123 [==============================] - 61s 498ms/step - loss: 0.0236 - accuracy: 0.8676 - val_loss: 0.0807 - val_accuracy: 0.7668\n",
      "Epoch 21/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.8722\n",
      "Epoch 21: accuracy improved from 0.87046 to 0.87225, saving model to /content/drive/MyDrive/thesis/cnn_models/VGG16Modelv1.h5\n",
      "123/123 [==============================] - 64s 523ms/step - loss: 0.0250 - accuracy: 0.8722 - val_loss: 0.0790 - val_accuracy: 0.7462\n",
      "Epoch 22/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.8792\n",
      "Epoch 22: accuracy improved from 0.87225 to 0.87916, saving model to /content/drive/MyDrive/thesis/cnn_models/VGG16Modelv1.h5\n",
      "123/123 [==============================] - 65s 527ms/step - loss: 0.0254 - accuracy: 0.8792 - val_loss: 0.0703 - val_accuracy: 0.7596\n",
      "Epoch 23/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.8833\n",
      "Epoch 23: accuracy improved from 0.87916 to 0.88326, saving model to /content/drive/MyDrive/thesis/cnn_models/VGG16Modelv1.h5\n",
      "123/123 [==============================] - 64s 523ms/step - loss: 0.0249 - accuracy: 0.8833 - val_loss: 0.0710 - val_accuracy: 0.7784\n",
      "Epoch 24/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.8986\n",
      "Epoch 24: accuracy improved from 0.88326 to 0.89862, saving model to /content/drive/MyDrive/thesis/cnn_models/VGG16Modelv1.h5\n",
      "123/123 [==============================] - 67s 543ms/step - loss: 0.0176 - accuracy: 0.8986 - val_loss: 0.0671 - val_accuracy: 0.7909\n",
      "Epoch 25/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.9173\n",
      "Epoch 25: accuracy improved from 0.89862 to 0.91731, saving model to /content/drive/MyDrive/thesis/cnn_models/VGG16Modelv1.h5\n",
      "123/123 [==============================] - 62s 503ms/step - loss: 0.0114 - accuracy: 0.9173 - val_loss: 0.0702 - val_accuracy: 0.7855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7cc91c7f9e40>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(data_generator(train_directory_path, batch_size, input_shape),\n",
    "          steps_per_epoch=len(os.listdir(train_directory_path)) // batch_size,\n",
    "          epochs=25,\n",
    "          verbose=1,\n",
    "          validation_data=data_generator(valid_directory_path, batch_size, input_shape),\n",
    "          validation_steps=len(os.listdir(valid_directory_path)) // batch_size,\n",
    "          callbacks=[save_best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1MPM94-gGWs8Y1-txDePbZ2PzwchYteK5"
    },
    "executionInfo": {
     "elapsed": 18121,
     "status": "ok",
     "timestamp": 1690630039830,
     "user": {
      "displayName": "TT CR",
      "userId": "00594919096815658785"
     },
     "user_tz": -360
    },
    "id": "lBX0xCeJLBQF",
    "outputId": "db58d225-b230-4410-87c5-8e7899bb3b29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a data generator that yields batches of test images and their corresponding annotations\n",
    "def test_data_generator(directory_path, batch_size, input_shape):\n",
    "    while True:\n",
    "        data = []\n",
    "        test_annotation = []\n",
    "        for filename in os.listdir(directory_path):\n",
    "            if filename.endswith('.jpg'):\n",
    "                # Extract the file path of the JPG image\n",
    "                image_path = os.path.join(directory_path, filename)\n",
    "                # Construct the file path of the corresponding XML annotation file\n",
    "                xml_path = os.path.join(directory_path, filename[:-4] + '.xml')\n",
    "\n",
    "                # Parse the XML annotation file\n",
    "                tree = ET.parse(xml_path)\n",
    "                root = tree.getroot()\n",
    "                # Check if the bndbox element is present\n",
    "                bndbox = root.find('object/bndbox')\n",
    "                if bndbox is not None:\n",
    "                    # Extract the bounding box coordinates\n",
    "                    xmin = int(bndbox.find('xmin').text)\n",
    "                    xmax = int(bndbox.find('xmax').text)\n",
    "                    ymin = int(bndbox.find('ymin').text)\n",
    "                    ymax = int(bndbox.find('ymax').text)\n",
    "                    # Normalize the bounding box coordinates to the range [0, 1]\n",
    "                    test_annotation.append([xmin / input_shape[0], ymin / input_shape[1],\n",
    "                                            xmax / input_shape[0], ymax / input_shape[1]])\n",
    "\n",
    "                    # Load the image\n",
    "                    image = cv2.resize(cv2.imread(image_path), input_shape[:2])\n",
    "                    data.append(image)\n",
    "\n",
    "                    # Yield the batch when the data and test_annotation lists are of length batch_size\n",
    "                    if len(data) == batch_size:\n",
    "                        yield np.array(data), np.array(test_annotation)\n",
    "                        data = []\n",
    "                        test_annotation = []\n",
    "\n",
    "        # Yield the last batch if it is smaller than batch_size\n",
    "        if len(data) > 0:\n",
    "            yield np.array(data), np.array(test_annotation)\n",
    "\n",
    "\n",
    "# Load the saved model\n",
    "model_path = '/content/drive/MyDrive/thesis/cnn_models/VGG16Modelv1.h5'\n",
    "Model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Define the path to the directory containing the test JPG files\n",
    "test_directory_path = r'/content/drive/MyDrive/thesis/dataset/roboflow-cnn/test'\n",
    "\n",
    "# Assuming you have 32 test samples, change this to the actual number of test samples\n",
    "num_test_samples = 32\n",
    "\n",
    "# Initialize the data generator for the test set\n",
    "batch_size = 1  # Set the batch size for testing (you can adjust it as needed)\n",
    "input_shape = (224, 224)  # Set the input shape based on the model's input requirements\n",
    "# Get the list of image file names in the test directory\n",
    "test_image_files = [filename for filename in os.listdir(test_directory_path) if filename.endswith('.jpg')]\n",
    "\n",
    "test_generator = test_data_generator(test_directory_path, batch_size, input_shape)\n",
    "\n",
    "# Loop through the test dataset and make predictions\n",
    "for i in range(num_test_samples):\n",
    "    images, annotations = next(test_generator)\n",
    "\n",
    "    predictions = Model.predict(images)\n",
    "\n",
    "    image = images[0].astype('uint8')  # Assuming batch size is 1\n",
    "    annotation = annotations[0]\n",
    "    pred = predictions[0]\n",
    "\n",
    "    # Convert the normalized coordinates to pixel values for 224x224 input shape\n",
    "    xmin, ymin, xmax, ymax = [int(anno * input_shape[i // 2]) for i, anno in enumerate(annotation)]\n",
    "    xmin1, ymin1, xmax1, ymax1 = [int(p * input_shape[i // 2]) for i, p in enumerate(pred)]\n",
    "\n",
    "    # Determine the class label based on the prediction score\n",
    "    class_label = \"Violence\" if pred[0] > 0.5 else \"NonViolence\"\n",
    "\n",
    "    # Set the color of the bounding box based on the class label\n",
    "    box_color = (0, 255, 0)  # Green color for NonViolence\n",
    "    if class_label == \"Violence\":\n",
    "        box_color = (255, 0, 0)  # Red color for Violence\n",
    "\n",
    "    cv2.rectangle(image, (xmin1, ymin1), (xmax1, ymax1), box_color, 2)\n",
    "    cv2.putText(image, class_label, (xmin1, ymin1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, box_color, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title('Prediction from VGG16Modelv1')\n",
    "    plt.show()\n",
    "\n",
    "    box1 = [xmin, ymin, xmax, ymax]  # coordinates of the ground truth bounding box\n",
    "    box2 = [xmin1, ymin1, xmax1, ymax1]  # coordinates of the predicted bounding box\n",
    "\n",
    "    # Function to calculate the Intersection over Union (IoU)\n",
    "    def calculate_iou(box1, box2):\n",
    "        x5 = max(box1[0], box2[0])\n",
    "        y5 = max(box1[1], box2[1])\n",
    "        x6 = min(box1[2], box2[2])\n",
    "        y6 = min(box1[3], box2[3])\n",
    "\n",
    "        # calculate the area of intersection\n",
    "        inter_area = max(0, x6 - x5) * max(0, y6 - y5)\n",
    "\n",
    "        # calculate the area of union\n",
    "        box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "        box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "        union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "        # calculate the IoU\n",
    "        iou = inter_area / union_area\n",
    "        return iou\n",
    "\n",
    "    iou = calculate_iou(box1, box2)\n",
    "    print(f\"IoU for image {i+1}: {iou}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7817,
     "status": "ok",
     "timestamp": 1690630943147,
     "user": {
      "displayName": "TT CR",
      "userId": "00594919096815658785"
     },
     "user_tz": -360
    },
    "id": "9dwH0QPXa7-j",
    "outputId": "79664462-2898-4d3d-e239-602d2e70d1bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Define a data generator that yields batches of frames and their corresponding annotations\n",
    "def video_data_generator(video_path, batch_size, input_shape):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while True:\n",
    "        data = []\n",
    "        video_frame = []\n",
    "        while len(data) < batch_size:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break  # Break the loop if the video ends\n",
    "\n",
    "            video_frame.append(frame)\n",
    "            if len(video_frame) == batch_size:\n",
    "                # Preprocess the frames and add them to the data list\n",
    "                data.extend([cv2.resize(frame, input_shape[:2]) for frame in video_frame])\n",
    "                video_frame = []\n",
    "\n",
    "        if len(data) == 0:\n",
    "            break\n",
    "\n",
    "        yield np.array(data)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "# Load the saved model\n",
    "model_path = '/content/drive/MyDrive/thesis/cnn_models/VGG16Modelv1.h5'\n",
    "Model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Define the path to the video file\n",
    "video_path = '/content/drive/MyDrive/thesis/dataset/mixed_videos/NV_1.mp4'\n",
    "\n",
    "# Initialize the data generator for the video\n",
    "batch_size = 1  # Set the batch size for testing (you can adjust it as needed)\n",
    "input_shape = (224, 224)  # Set the input shape based on the model's input requirements\n",
    "\n",
    "video_generator = video_data_generator(video_path, batch_size, input_shape)\n",
    "\n",
    "# Define the output video path\n",
    "output_video_path = '/content/drive/MyDrive/thesis/cnn_models/NV_1_output.mp4'\n",
    "\n",
    "# Initialize the VideoWriter\n",
    "output_shape = (input_shape[1], input_shape[0])  # Width, Height\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "output_video = cv2.VideoWriter(output_video_path, fourcc, 30.0, output_shape)\n",
    "\n",
    "# Loop through the video frames and make predictions\n",
    "frame_idx = 0\n",
    "try:\n",
    "    while True:\n",
    "        frames = next(video_generator)\n",
    "        if len(frames) == 0:\n",
    "            break  # Break the loop if there are no more frames\n",
    "\n",
    "        predictions = Model.predict(frames)\n",
    "\n",
    "        for frame_idx, frame in enumerate(frames):\n",
    "            pred = predictions[frame_idx]\n",
    "\n",
    "            # Convert the normalized coordinates to pixel values for 224x224 input shape\n",
    "            xmin, ymin, xmax, ymax = [int(p * input_shape[i // 2]) for i, p in enumerate(pred)]\n",
    "\n",
    "            # Determine the class label based on the prediction score\n",
    "            class_label = \"Violence\" if pred[0] > 0.5 else \"NonViolence\"\n",
    "\n",
    "            # Set the color of the bounding box based on the class label\n",
    "            box_color = (0, 255, 0)  # Green color for NonViolence\n",
    "            if class_label == \"Violence\":\n",
    "                box_color = (255, 0, 0)  # Red color for Violence\n",
    "\n",
    "            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), box_color, 2)\n",
    "            cv2.putText(frame, class_label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, box_color, 1)\n",
    "\n",
    "            # Write the frame with annotations to the output video\n",
    "            output_video.write(frame)\n",
    "\n",
    "except StopIteration:\n",
    "    pass\n",
    "\n",
    "# Release the video writer and close the output video file\n",
    "output_video.release()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN16uWKWLCr+kfCglEfS+EU",
   "gpuType": "T4",
   "mount_file_id": "12XPoeksAuhbUJCF002IrimcfOcRIgNOU",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
