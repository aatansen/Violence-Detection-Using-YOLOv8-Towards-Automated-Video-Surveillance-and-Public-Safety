{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2906885,
     "status": "ok",
     "timestamp": 1690703408992,
     "user": {
      "displayName": "Md. Alahi Almin Tansen",
      "userId": "06335464451393184265"
     },
     "user_tz": -360
    },
    "id": "TcwUENAeiaV9",
    "outputId": "76b26c47-94a7-492d-de6c-2becbf9bb53a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.6943 - accuracy: 0.5843\n",
      "Epoch 1: accuracy improved from -inf to 0.58429, saving model to /content/drive/MyDrive/thesis/cnn_models/ResNet152V2Modelv1.h5\n",
      "Total training time for epoch 1: 228.27 seconds\n",
      "Highest training accuracy: 0.5843 at epoch 1\n",
      "Highest validation accuracy: 0.6184 at epoch 1\n",
      "123/123 [==============================] - 228s 1s/step - loss: 0.6943 - accuracy: 0.5843 - val_loss: 0.4648 - val_accuracy: 0.6184\n",
      "Epoch 2/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0607 - accuracy: 0.8692\n",
      "Epoch 2: accuracy improved from 0.58429 to 0.86918, saving model to /content/drive/MyDrive/thesis/cnn_models/ResNet152V2Modelv1.h5\n",
      "Total training time for epoch 2: 134.05 seconds\n",
      "Highest training accuracy: 0.8692 at epoch 2\n",
      "Highest validation accuracy: 0.6184 at epoch 1\n",
      "123/123 [==============================] - 134s 1s/step - loss: 0.0607 - accuracy: 0.8692 - val_loss: 0.2567 - val_accuracy: 0.6166\n",
      "Epoch 3/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0929 - accuracy: 0.8656\n",
      "Epoch 3: accuracy did not improve from 0.86918\n",
      "Total training time for epoch 3: 107.68 seconds\n",
      "Highest training accuracy: 0.8692 at epoch 2\n",
      "Highest validation accuracy: 0.6720 at epoch 3\n",
      "123/123 [==============================] - 108s 876ms/step - loss: 0.0929 - accuracy: 0.8656 - val_loss: 0.1517 - val_accuracy: 0.6720\n",
      "Epoch 4/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0422 - accuracy: 0.9060\n",
      "Epoch 4: accuracy improved from 0.86918 to 0.90604, saving model to /content/drive/MyDrive/thesis/cnn_models/ResNet152V2Modelv1.h5\n",
      "Total training time for epoch 4: 129.44 seconds\n",
      "Highest training accuracy: 0.9060 at epoch 4\n",
      "Highest validation accuracy: 0.7131 at epoch 4\n",
      "123/123 [==============================] - 129s 1s/step - loss: 0.0422 - accuracy: 0.9060 - val_loss: 0.1253 - val_accuracy: 0.7131\n",
      "Epoch 5/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.8971\n",
      "Epoch 5: accuracy did not improve from 0.90604\n",
      "Total training time for epoch 5: 107.83 seconds\n",
      "Highest training accuracy: 0.9060 at epoch 4\n",
      "Highest validation accuracy: 0.7131 at epoch 4\n",
      "123/123 [==============================] - 108s 877ms/step - loss: 0.0329 - accuracy: 0.8971 - val_loss: 0.2027 - val_accuracy: 0.6524\n",
      "Epoch 6/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.8597\n",
      "Epoch 6: accuracy did not improve from 0.90604\n",
      "Total training time for epoch 6: 107.66 seconds\n",
      "Highest training accuracy: 0.9060 at epoch 4\n",
      "Highest validation accuracy: 0.7131 at epoch 4\n",
      "123/123 [==============================] - 108s 876ms/step - loss: 0.0732 - accuracy: 0.8597 - val_loss: 0.3185 - val_accuracy: 0.5639\n",
      "Epoch 7/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.1253 - accuracy: 0.8039\n",
      "Epoch 7: accuracy did not improve from 0.90604\n",
      "Total training time for epoch 7: 104.41 seconds\n",
      "Highest training accuracy: 0.9060 at epoch 4\n",
      "Highest validation accuracy: 0.7319 at epoch 7\n",
      "123/123 [==============================] - 104s 850ms/step - loss: 0.1253 - accuracy: 0.8039 - val_loss: 1.1137 - val_accuracy: 0.7319\n",
      "Epoch 8/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0808 - accuracy: 0.8024\n",
      "Epoch 8: accuracy did not improve from 0.90604\n",
      "Total training time for epoch 8: 107.28 seconds\n",
      "Highest training accuracy: 0.9060 at epoch 4\n",
      "Highest validation accuracy: 0.7444 at epoch 8\n",
      "123/123 [==============================] - 107s 873ms/step - loss: 0.0808 - accuracy: 0.8024 - val_loss: 0.1482 - val_accuracy: 0.7444\n",
      "Epoch 9/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0513 - accuracy: 0.8351\n",
      "Epoch 9: accuracy did not improve from 0.90604\n",
      "Total training time for epoch 9: 107.26 seconds\n",
      "Highest training accuracy: 0.9060 at epoch 4\n",
      "Highest validation accuracy: 0.7444 at epoch 8\n",
      "123/123 [==============================] - 107s 873ms/step - loss: 0.0513 - accuracy: 0.8351 - val_loss: 0.1511 - val_accuracy: 0.5478\n",
      "Epoch 10/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.8751\n",
      "Epoch 10: accuracy did not improve from 0.90604\n",
      "Total training time for epoch 10: 105.35 seconds\n",
      "Highest training accuracy: 0.9060 at epoch 4\n",
      "Highest validation accuracy: 0.7748 at epoch 10\n",
      "123/123 [==============================] - 105s 857ms/step - loss: 0.0344 - accuracy: 0.8751 - val_loss: 0.0909 - val_accuracy: 0.7748\n",
      "Epoch 11/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9196\n",
      "Epoch 11: accuracy improved from 0.90604 to 0.91961, saving model to /content/drive/MyDrive/thesis/cnn_models/ResNet152V2Modelv1.h5\n",
      "Total training time for epoch 11: 130.75 seconds\n",
      "Highest training accuracy: 0.9196 at epoch 11\n",
      "Highest validation accuracy: 0.7748 at epoch 10\n",
      "123/123 [==============================] - 131s 1s/step - loss: 0.0253 - accuracy: 0.9196 - val_loss: 0.0824 - val_accuracy: 0.7650\n",
      "Epoch 12/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9350\n",
      "Epoch 12: accuracy improved from 0.91961 to 0.93497, saving model to /content/drive/MyDrive/thesis/cnn_models/ResNet152V2Modelv1.h5\n",
      "Total training time for epoch 12: 136.30 seconds\n",
      "Highest training accuracy: 0.9350 at epoch 12\n",
      "Highest validation accuracy: 0.7748 at epoch 10\n",
      "123/123 [==============================] - 136s 1s/step - loss: 0.0149 - accuracy: 0.9350 - val_loss: 0.1134 - val_accuracy: 0.7471\n",
      "Epoch 13/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9347\n",
      "Epoch 13: accuracy did not improve from 0.93497\n",
      "Total training time for epoch 13: 105.63 seconds\n",
      "Highest training accuracy: 0.9350 at epoch 12\n",
      "Highest validation accuracy: 0.7748 at epoch 10\n",
      "123/123 [==============================] - 106s 859ms/step - loss: 0.0141 - accuracy: 0.9347 - val_loss: 0.0979 - val_accuracy: 0.7480\n",
      "Epoch 14/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9283\n",
      "Epoch 14: accuracy did not improve from 0.93497\n",
      "Total training time for epoch 14: 105.12 seconds\n",
      "Highest training accuracy: 0.9350 at epoch 12\n",
      "Highest validation accuracy: 0.7748 at epoch 10\n",
      "123/123 [==============================] - 105s 856ms/step - loss: 0.0164 - accuracy: 0.9283 - val_loss: 0.0954 - val_accuracy: 0.7319\n",
      "Epoch 15/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9283\n",
      "Epoch 15: accuracy did not improve from 0.93497\n",
      "Total training time for epoch 15: 103.30 seconds\n",
      "Highest training accuracy: 0.9350 at epoch 12\n",
      "Highest validation accuracy: 0.7748 at epoch 10\n",
      "123/123 [==============================] - 103s 840ms/step - loss: 0.0132 - accuracy: 0.9283 - val_loss: 0.1143 - val_accuracy: 0.7399\n",
      "Epoch 16/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9250\n",
      "Epoch 16: accuracy did not improve from 0.93497\n",
      "Total training time for epoch 16: 105.87 seconds\n",
      "Highest training accuracy: 0.9350 at epoch 12\n",
      "Highest validation accuracy: 0.7748 at epoch 10\n",
      "123/123 [==============================] - 106s 862ms/step - loss: 0.0175 - accuracy: 0.9250 - val_loss: 0.0817 - val_accuracy: 0.7596\n",
      "Epoch 17/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9012\n",
      "Epoch 17: accuracy did not improve from 0.93497\n",
      "Total training time for epoch 17: 105.79 seconds\n",
      "Highest training accuracy: 0.9350 at epoch 12\n",
      "Highest validation accuracy: 0.7748 at epoch 10\n",
      "123/123 [==============================] - 106s 861ms/step - loss: 0.0231 - accuracy: 0.9012 - val_loss: 0.1157 - val_accuracy: 0.7283\n",
      "Epoch 18/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 0.9168\n",
      "Epoch 18: accuracy did not improve from 0.93497\n",
      "Total training time for epoch 18: 104.15 seconds\n",
      "Highest training accuracy: 0.9350 at epoch 12\n",
      "Highest validation accuracy: 0.7748 at epoch 10\n",
      "123/123 [==============================] - 104s 848ms/step - loss: 0.0295 - accuracy: 0.9168 - val_loss: 0.1286 - val_accuracy: 0.6139\n",
      "Epoch 19/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.8902\n",
      "Epoch 19: accuracy did not improve from 0.93497\n",
      "Total training time for epoch 19: 105.42 seconds\n",
      "Highest training accuracy: 0.9350 at epoch 12\n",
      "Highest validation accuracy: 0.7748 at epoch 10\n",
      "123/123 [==============================] - 105s 858ms/step - loss: 0.0403 - accuracy: 0.8902 - val_loss: 0.0962 - val_accuracy: 0.7489\n",
      "Epoch 20/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9071\n",
      "Epoch 20: accuracy did not improve from 0.93497\n",
      "Total training time for epoch 20: 105.88 seconds\n",
      "Highest training accuracy: 0.9350 at epoch 12\n",
      "Highest validation accuracy: 0.7748 at epoch 10\n",
      "123/123 [==============================] - 106s 862ms/step - loss: 0.0310 - accuracy: 0.9071 - val_loss: 0.0951 - val_accuracy: 0.7453\n",
      "Epoch 21/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.8848\n",
      "Epoch 21: accuracy did not improve from 0.93497\n",
      "Total training time for epoch 21: 104.30 seconds\n",
      "Highest training accuracy: 0.9350 at epoch 12\n",
      "Highest validation accuracy: 0.7873 at epoch 21\n",
      "123/123 [==============================] - 104s 848ms/step - loss: 0.0288 - accuracy: 0.8848 - val_loss: 0.1976 - val_accuracy: 0.7873\n",
      "Epoch 22/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9009\n",
      "Epoch 22: accuracy did not improve from 0.93497\n",
      "Total training time for epoch 22: 105.48 seconds\n",
      "Highest training accuracy: 0.9350 at epoch 12\n",
      "Highest validation accuracy: 0.7873 at epoch 21\n",
      "123/123 [==============================] - 105s 859ms/step - loss: 0.0233 - accuracy: 0.9009 - val_loss: 0.1585 - val_accuracy: 0.7185\n",
      "Epoch 23/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9232\n",
      "Epoch 23: accuracy did not improve from 0.93497\n",
      "Total training time for epoch 23: 104.51 seconds\n",
      "Highest training accuracy: 0.9350 at epoch 12\n",
      "Highest validation accuracy: 0.7882 at epoch 23\n",
      "123/123 [==============================] - 105s 850ms/step - loss: 0.0175 - accuracy: 0.9232 - val_loss: 0.0696 - val_accuracy: 0.7882\n",
      "Epoch 24/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.9332\n",
      "Epoch 24: accuracy did not improve from 0.93497\n",
      "Total training time for epoch 24: 104.06 seconds\n",
      "Highest training accuracy: 0.9350 at epoch 12\n",
      "Highest validation accuracy: 0.8097 at epoch 24\n",
      "123/123 [==============================] - 104s 847ms/step - loss: 0.0108 - accuracy: 0.9332 - val_loss: 0.0694 - val_accuracy: 0.8097\n",
      "Epoch 25/25\n",
      "123/123 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 0.9483\n",
      "Epoch 25: accuracy improved from 0.93497 to 0.94828, saving model to /content/drive/MyDrive/thesis/cnn_models/ResNet152V2Modelv1.h5\n",
      "Total training time for epoch 25: 128.41 seconds\n",
      "Highest training accuracy: 0.9483 at epoch 25\n",
      "Highest validation accuracy: 0.8097 at epoch 24\n",
      "123/123 [==============================] - 128s 1s/step - loss: 0.0094 - accuracy: 0.9483 - val_loss: 0.0891 - val_accuracy: 0.7453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ace68725f30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet152V2\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input\n",
    "from google.colab.patches import cv2_imshow  # Import cv2_imshow for displaying images\n",
    "import time\n",
    "\n",
    "# Define the path to the directory containing the JPG and XML files\n",
    "train_directory_path = r'/content/drive/MyDrive/thesis/dataset/roboflow-cnn/train'\n",
    "valid_directory_path = r'/content/drive/MyDrive/thesis/dataset/roboflow-cnn/valid'\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Define the input shape of the model\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "# Load the ResNet152V2 model\n",
    "base_model = ResNet152V2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "# Add a custom output layer for bounding box regression (4 values)\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(4, activation='linear')(x)  # 4 values for xmin, ymin, xmax, ymax\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
    "save_best = tf.keras.callbacks.ModelCheckpoint(\"/content/drive/MyDrive/thesis/cnn_models/ResNet152V2Modelv1.h5\", monitor='accuracy', save_best_only=True, verbose=1)\n",
    "\n",
    "# Define a callback to track and print total training time and highest accuracy\n",
    "class TimingCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.total_time = 0.0\n",
    "        self.highest_train_acc = 0.0\n",
    "        self.highest_val_acc = 0.0\n",
    "        self.highest_train_epoch = 0\n",
    "        self.highest_val_epoch = 0\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch_time = time.time() - self.start_time\n",
    "        self.total_time += epoch_time\n",
    "        print(f'Total training time for epoch {epoch + 1}: {epoch_time:.2f} seconds')\n",
    "\n",
    "        train_acc = logs['accuracy']\n",
    "        val_acc = logs['val_accuracy']\n",
    "\n",
    "        if train_acc > self.highest_train_acc:\n",
    "            self.highest_train_acc = train_acc\n",
    "            self.highest_train_epoch = epoch + 1\n",
    "\n",
    "        if val_acc > self.highest_val_acc:\n",
    "            self.highest_val_acc = val_acc\n",
    "            self.highest_val_epoch = epoch + 1\n",
    "\n",
    "        print(f'Highest training accuracy: {self.highest_train_acc:.4f} at epoch {self.highest_train_epoch}')\n",
    "        print(f'Highest validation accuracy: {self.highest_val_acc:.4f} at epoch {self.highest_val_epoch}')\n",
    "\n",
    "# Define a data generator that yields batches of images and their corresponding annotations\n",
    "def data_generator(directory_path, batch_size, input_shape):\n",
    "    while True:\n",
    "        data = []\n",
    "        train_annotation = []\n",
    "        for filename in os.listdir(directory_path):\n",
    "            if filename.endswith('.jpg'):\n",
    "                # Extract the file path of the JPG image\n",
    "                image_path = os.path.join(directory_path, filename)\n",
    "                # Construct the file path of the corresponding XML annotation file\n",
    "                xml_path = os.path.join(directory_path, filename[:-4] + '.xml')\n",
    "\n",
    "                # Parse the XML annotation file\n",
    "                tree = ET.parse(xml_path)\n",
    "                root = tree.getroot()\n",
    "                # Check if the bndbox element is present\n",
    "                bndbox = root.find('object/bndbox')\n",
    "                if bndbox is not None:\n",
    "                    # Extract the bounding box coordinates\n",
    "                    xmin = int(bndbox.find('xmin').text)\n",
    "                    xmax = int(bndbox.find('xmax').text)\n",
    "                    ymin = int(bndbox.find('ymin').text)\n",
    "                    ymax = int(bndbox.find('ymax').text)\n",
    "                    # Normalize the bounding box coordinates to the range [0, 1]\n",
    "                    train_annotation.append([xmin / input_shape[0], ymin / input_shape[1],\n",
    "                                             xmax / input_shape[0], ymax / input_shape[1]])\n",
    "\n",
    "                    # Load the image and preprocess it\n",
    "                    image = cv2.resize(cv2.imread(image_path), input_shape[:2])\n",
    "                    image = preprocess_input(image)\n",
    "                    data.append(image)\n",
    "\n",
    "                    # Yield the batch when the data and train_annotation lists are of length batch_size\n",
    "                    if len(data) == batch_size:\n",
    "                        yield np.array(data), np.array(train_annotation)\n",
    "                        data = []\n",
    "                        train_annotation = []\n",
    "\n",
    "        # Yield the last batch if it is smaller than batch_size\n",
    "        if len(data) > 0:\n",
    "            yield np.array(data), np.array(train_annotation)\n",
    "\n",
    "# Train the model with the timing callback\n",
    "timing_callback = TimingCallback()\n",
    "history = model.fit(data_generator(train_directory_path, batch_size, input_shape),\n",
    "                    steps_per_epoch=len(os.listdir(train_directory_path)) // batch_size,\n",
    "                    epochs=25,\n",
    "                    verbose=1,\n",
    "                    validation_data=data_generator(valid_directory_path, batch_size, input_shape),\n",
    "                    validation_steps=len(os.listdir(valid_directory_path)) // batch_size,\n",
    "                    callbacks=[save_best, timing_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "146LlKyrGte7josLAtn03rw29t-9GKjwu"
    },
    "executionInfo": {
     "elapsed": 28259,
     "status": "ok",
     "timestamp": 1690704658296,
     "user": {
      "displayName": "Md. Alahi Almin Tansen",
      "userId": "06335464451393184265"
     },
     "user_tz": -360
    },
    "id": "snWM6CZPxyLa",
    "outputId": "331f314b-0a11-4443-a5ba-090b6694514f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a data generator that yields batches of test images and their corresponding annotations\n",
    "def test_data_generator(directory_path, batch_size, input_shape):\n",
    "    while True:\n",
    "        data = []\n",
    "        test_annotation = []\n",
    "        for filename in os.listdir(directory_path):\n",
    "            if filename.endswith('.jpg'):\n",
    "                # Extract the file path of the JPG image\n",
    "                image_path = os.path.join(directory_path, filename)\n",
    "                # Construct the file path of the corresponding XML annotation file\n",
    "                xml_path = os.path.join(directory_path, filename[:-4] + '.xml')\n",
    "\n",
    "                # Parse the XML annotation file\n",
    "                tree = ET.parse(xml_path)\n",
    "                root = tree.getroot()\n",
    "                # Check if the bndbox element is present\n",
    "                bndbox = root.find('object/bndbox')\n",
    "                if bndbox is not None:\n",
    "                    # Extract the bounding box coordinates\n",
    "                    xmin = int(bndbox.find('xmin').text)\n",
    "                    xmax = int(bndbox.find('xmax').text)\n",
    "                    ymin = int(bndbox.find('ymin').text)\n",
    "                    ymax = int(bndbox.find('ymax').text)\n",
    "                    # Normalize the bounding box coordinates to the range [0, 1]\n",
    "                    test_annotation.append([xmin / input_shape[0], ymin / input_shape[1],\n",
    "                                            xmax / input_shape[0], ymax / input_shape[1]])\n",
    "\n",
    "                    # Load the image\n",
    "                    image = cv2.resize(cv2.imread(image_path), input_shape[:2])\n",
    "                    data.append(image)\n",
    "\n",
    "                    # Yield the batch when the data and test_annotation lists are of length batch_size\n",
    "                    if len(data) == batch_size:\n",
    "                        yield np.array(data), np.array(test_annotation)\n",
    "                        data = []\n",
    "                        test_annotation = []\n",
    "\n",
    "        # Yield the last batch if it is smaller than batch_size\n",
    "        if len(data) > 0:\n",
    "            yield np.array(data), np.array(test_annotation)\n",
    "\n",
    "\n",
    "# Load the saved model\n",
    "model_path = '/content/drive/MyDrive/thesis/cnn_models/ResNet152V2Modelv1.h5'\n",
    "Model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Define the path to the directory containing the test JPG files\n",
    "test_directory_path = r'/content/drive/MyDrive/thesis/dataset/roboflow-cnn/test'\n",
    "\n",
    "# Assuming you have 32 test samples, change this to the actual number of test samples\n",
    "num_test_samples = 32\n",
    "\n",
    "# Initialize the data generator for the test set\n",
    "batch_size = 1  # Set the batch size for testing (you can adjust it as needed)\n",
    "input_shape = (224, 224)  # Set the input shape based on the model's input requirements\n",
    "# Get the list of image file names in the test directory\n",
    "test_image_files = [filename for filename in os.listdir(test_directory_path) if filename.endswith('.jpg')]\n",
    "\n",
    "test_generator = test_data_generator(test_directory_path, batch_size, input_shape)\n",
    "\n",
    "# Loop through the test dataset and make predictions\n",
    "for i in range(num_test_samples):\n",
    "    images, annotations = next(test_generator)\n",
    "\n",
    "    predictions = Model.predict(images)\n",
    "\n",
    "    image = images[0].astype('uint8')  # Assuming batch size is 1\n",
    "    annotation = annotations[0]\n",
    "    pred = predictions[0]\n",
    "\n",
    "    # Convert the normalized coordinates to pixel values for 224x224 input shape\n",
    "    xmin, ymin, xmax, ymax = [int(anno * input_shape[i // 2]) for i, anno in enumerate(annotation)]\n",
    "    xmin1, ymin1, xmax1, ymax1 = [int(p * input_shape[i // 2]) for i, p in enumerate(pred)]\n",
    "\n",
    "    # Determine the class label based on the prediction score\n",
    "    class_label = \"Violence\" if pred[0] > 0.5 else \"NonViolence\"\n",
    "\n",
    "    # Set the color of the bounding box based on the class label\n",
    "    box_color = (0, 255, 0)  # Green color for NonViolence\n",
    "    if class_label == \"Violence\":\n",
    "        box_color = (255, 0, 0)  # Red color for Violence\n",
    "\n",
    "    cv2.rectangle(image, (xmin1, ymin1), (xmax1, ymax1), box_color, 2)\n",
    "    cv2.putText(image, class_label, (xmin1, ymin1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, box_color, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title('Prediction from ResNet152V2Modelv1')\n",
    "    plt.show()\n",
    "\n",
    "    box1 = [xmin, ymin, xmax, ymax]  # coordinates of the ground truth bounding box\n",
    "    box2 = [xmin1, ymin1, xmax1, ymax1]  # coordinates of the predicted bounding box\n",
    "\n",
    "    # Function to calculate the Intersection over Union (IoU)\n",
    "    def calculate_iou(box1, box2):\n",
    "        x5 = max(box1[0], box2[0])\n",
    "        y5 = max(box1[1], box2[1])\n",
    "        x6 = min(box1[2], box2[2])\n",
    "        y6 = min(box1[3], box2[3])\n",
    "\n",
    "        # calculate the area of intersection\n",
    "        inter_area = max(0, x6 - x5) * max(0, y6 - y5)\n",
    "\n",
    "        # calculate the area of union\n",
    "        box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "        box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "        union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "        # calculate the IoU\n",
    "        iou = inter_area / union_area\n",
    "        return iou\n",
    "\n",
    "    iou = calculate_iou(box1, box2)\n",
    "    print(f\"IoU for image {i+1}: {iou}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqdcGEvyyMFN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOcxZdhMb4GUafuJ6iWdejm",
   "gpuType": "T4",
   "mount_file_id": "1Zvzqv2m4CssAgle6XbA5oCnVQ7moVSAi",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
